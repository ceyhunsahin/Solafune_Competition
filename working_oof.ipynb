{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import glob\n",
    "import tifffile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from rasterio.plot import show\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, AdaBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.svm import NuSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
    "from catboost import Pool\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.inspection import permutation_importance\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "from colorama import Style, Fore\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:03.473010Z",
     "start_time": "2023-11-26T15:30:02.739970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_path =  'train/s2_image/'\n",
    "mask_path = 'train/mask/'\n",
    "\n",
    "masks = glob.glob(f'{mask_path}/*')\n",
    "trains = glob.glob(f'{train_path}/*')\n",
    "masks.sort()\n",
    "trains.sort()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:03.523748Z",
     "start_time": "2023-11-26T15:30:03.473169Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:03.550771Z",
     "start_time": "2023-11-26T15:30:03.519747Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_path =  'train/s2_image/'\n",
    "mask_path = 'train/mask/'\n",
    "\n",
    "masks = glob.glob(f'{mask_path}/*')\n",
    "trains = glob.glob(f'{train_path}/*')\n",
    "masks.sort()\n",
    "trains.sort()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:03.790302Z",
     "start_time": "2023-11-26T15:30:03.752614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "path = 'train/s2_image/train_s2_image_90.tif'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:03.960865Z",
     "start_time": "2023-11-26T15:30:03.953855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ds = rasterio.open(path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:04.282003Z",
     "start_time": "2023-11-26T15:30:04.278806Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Axes: >"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show((ds, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:04.691191Z",
     "start_time": "2023-11-26T15:30:04.482363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1392.82625192, 1427.61220048, 1549.00970358, 1438.80199676,\n        1252.12051541, 1124.77196345, 1002.60303541,  999.54622263,\n         960.8109608 ,  947.37164807,  827.65079312,  812.98968406,\n         805.83250276,  750.        ,  798.88313184,  936.5       ,\n        1064.19387354, 1247.5       , 1247.5       , 1273.78262084,\n        1289.        , 1401.57912117, 1433.        ],\n       [1406.        , 1441.97007643, 1567.5       , 1455.58072193,\n        1266.        , 1125.42344739,  995.5       ,  995.5       ,\n         963.57609315,  952.5       ,  834.45578628,  820.        ,\n         833.9535181 ,  942.80360943,  994.24010673, 1139.04526549,\n        1237.24032955, 1378.20055208, 1378.20055208, 1385.71746365,\n        1390.06968158, 1482.65674665, 1508.49783443],\n       [1867.12778561, 1804.46942346, 1585.8015697 , 1542.48527093,\n        1469.11153825, 1315.48043812, 1173.4917367 , 1173.4917367 ,\n        1071.76596351, 1036.47190804,  916.50947509,  901.81878219,\n         911.55428124,  987.5       , 1039.52842718, 1186.        ,\n        1277.3565494 , 1408.5       , 1408.5       , 1411.66658082,\n        1413.5       , 1501.45243842, 1526.        ],\n       [2048.5       , 1947.04879373, 1593.        , 1576.66683836,\n        1549.        , 1390.23424465, 1243.5       , 1243.5       ,\n        1114.31953971, 1069.5       ,  948.7830871 ,  934.        ,\n         965.587758  , 1212.00090771, 1235.91523175, 1303.23936291,\n        1402.91754675, 1546.00680597, 1546.00680597, 1583.73103878,\n        1605.57299881, 1560.67057018, 1548.13828395],\n       [1866.08045582, 1809.26849631, 1611.00367054, 1662.87754822,\n        1750.74701403, 1619.7751358 , 1498.72850593, 1498.72850593,\n        1215.63994248, 1117.42153483,  925.70640116,  912.10322479,\n        1175.06135439, 1347.5       , 1354.44586056, 1374.        ,\n        1478.70076448, 1629.        , 1629.        , 1687.58174525,\n        1721.5       , 1596.41208758, 1561.5       ],\n       [1704.        , 1686.85018028, 1627.        , 1739.4760904 ,\n        1930.        , 1823.72308684, 1725.5       , 1725.5       ,\n        1305.66350405, 1160.        ,  905.20267831,  893.1794582 ,\n        1360.99823339, 1360.99823339, 1381.86781314, 1440.62031319,\n        1540.85151614, 1684.7346411 , 1684.7346411 , 1721.53111851,\n        1742.8359173 , 1548.30303904, 1494.00883304],\n       [1679.42387152, 1682.34434574, 1692.53634262, 1772.13212191,\n        1906.95987955, 1831.63734021, 1762.02285761, 1762.02285761,\n        1406.04638789, 1282.5393073 , 1010.86469296,  993.38404725,\n        1378.5       , 1378.5       , 1417.42302991, 1527.        ,\n        1621.43598364, 1757.        , 1757.        , 1765.54976823,\n        1770.5       , 1485.92499925, 1406.5       ],\n       [1632.        , 1673.64956218, 1819.        , 1835.14755753,\n        1862.5       , 1846.90925479, 1832.5       , 1832.5       ,\n        1599.75244654, 1519.        , 1214.7577435 , 1185.37216826,\n        1377.38740009, 1377.38740009, 1422.17525789, 1548.26302051,\n        1624.37529519, 1733.63540188, 1733.63540188, 1730.67625414,\n        1728.96293668, 1500.25035155, 1436.41657537],\n       [1633.76128024, 1670.07240939, 1796.79255346, 1804.4122627 ,\n        1817.3193329 , 1815.3788302 , 1813.58538174, 1813.58538174,\n        1637.40588863, 1576.27989659, 1290.45779241, 1187.95619445,\n        1374.        , 1374.        , 1436.64379898, 1613.        ,\n        1633.32426605, 1662.5       , 1657.36250309, 1621.13146666,\n        1602.5       , 1543.86504106, 1527.5       ],\n       [1643.5       , 1650.29311041, 1674.        , 1634.46632466,\n        1567.5       , 1641.03634822, 1709.        , 1709.        ,\n        1845.60462468, 1893.        , 1709.02920654, 1231.08198819,\n        1380.79261938, 1380.79261938, 1442.32173344, 1615.53984898,\n        1631.25621856, 1653.81726045, 1622.1715794 , 1597.21357014,\n        1597.21357014, 1541.34928916, 1525.75754546],\n       [1643.5       , 1650.29311041, 1674.        , 1634.46632466,\n        1567.5       , 1641.03634822, 1709.        , 1709.        ,\n        1845.60462468, 1893.        , 1709.02920654, 1678.72185475,\n        1489.        , 1489.        , 1532.7720269 , 1656.        ,\n        1598.31193173, 1515.5       , 1514.1023137 , 1513.        ,\n        1513.        , 1501.27300821, 1498.        ],\n       [1784.8693958 , 1725.46565564, 1518.15592203, 1470.56256521,\n        1389.94389879, 1511.37266543, 1623.59937523, 1623.59937523,\n        1843.66647929, 1920.01940671, 1711.97703121, 1678.72185475,\n        1489.        , 1489.        , 1532.7720269 , 1656.        ,\n        1598.31193173, 1515.5       , 1514.1023137 , 1513.        ,\n        1513.        , 1501.27300821, 1498.        ],\n       [1790.        , 1728.19383153, 1512.5       , 1464.61413973,\n        1383.5       , 1506.66688713, 1620.5       , 1620.5       ,\n        1843.59613976, 1921.        , 1867.25959083, 1850.58521319,\n        1604.39325525, 1604.39325525, 1594.75060713, 1567.60441201,\n        1510.45271498, 1428.41075075, 1461.82597137, 1488.17956396,\n        1488.17956396, 1555.09228031, 1573.76764685],\n       [1720.47575146, 1685.14077711, 1561.82726014, 1542.34458747,\n        1509.3427739 , 1569.12874087, 1624.38403623, 1624.38403623,\n        1769.62372265, 1820.01505798, 1879.24647462, 1876.06350131,\n        1621.5       , 1621.5       , 1603.93876765, 1554.5       ,\n        1497.42781858, 1415.5       , 1454.07614195, 1484.5       ,\n        1484.5       , 1563.07084499, 1585.        ],\n       [1700.5       , 1672.77074604, 1576.        , 1564.67814932,\n        1545.5       , 1587.07532055, 1625.5       , 1625.5       ,\n        1748.36992057, 1791.        , 1849.31846072, 1849.76107534,\n        1686.3586919 , 1686.3586919 , 1661.99746816, 1593.41521514,\n        1555.40480108, 1500.84038407, 1524.53049611, 1543.21418424,\n        1543.21418424, 1628.99068831, 1652.93094572],\n       [1583.32009081, 1801.22920783, 1761.42159096, 1723.55101883,\n        1659.40183445, 1635.81807074, 1614.02152056, 1614.02152056,\n        1675.49069438, 1773.73241901, 1836.8813359 , 1837.53774025,\n        1716.5       , 1716.5       , 1688.97866572, 1611.5       ,\n        1582.34802244, 1540.5       , 1557.27223563, 1570.5       ,\n        1570.5       , 1659.6251376 , 1684.5       ],\n       [1501.43110407, 1891.        , 1891.        , 1834.57635069,\n        1739.        , 1669.88102959, 1606.        , 1606.        ,\n        1624.56041096, 1697.04373461, 1733.73928139, 1735.66213295,\n        1717.24183691, 1717.24183691, 1706.70171382, 1677.02892739,\n        1648.38460272, 1607.26532225, 1578.00118286, 1554.92142481,\n        1554.92142481, 1597.84246951, 1609.82175067],\n       [1483.44853446, 1839.93918427, 1839.93918427, 1804.47664414,\n        1744.40643931, 1702.75990137, 1664.26940148, 1664.26940148,\n        1627.82548336, 1628.09974336, 1628.32730082, 1631.54447125,\n        1718.        , 1718.        , 1724.81480658, 1744.        ,\n        1715.87450052, 1675.5       , 1599.18632789, 1539.        ,\n        1539.        , 1534.70010301, 1533.5       ],\n       [1456.52822517, 1763.5       , 1763.5       , 1759.41670959,\n        1752.5       , 1751.98030849, 1751.5       , 1751.5       ,\n        1632.71336985, 1613.87423829, 1615.6786771 , 1617.36890371,\n        1668.21047089, 1668.21047089, 1670.88935806, 1678.43102013,\n        1684.83870879, 1694.03702468, 1635.36713032, 1589.09592621,\n        1589.09592621, 1557.60811587, 1548.81985511],\n       [1456.14290855, 1723.1595248 , 1723.1595248 , 1725.22374936,\n        1728.72035146, 1740.88976137, 1752.13695487, 1736.0351945 ,\n        1626.67015784, 1591.5       , 1587.04550137, 1585.27912657,\n        1555.5       , 1555.5       , 1548.81624739, 1530.        ,\n        1614.58179405, 1736.        , 1717.27100355, 1702.5       ,\n        1702.5       , 1609.46586514, 1583.5       ],\n       [1454.71341862, 1573.5       , 1573.5       , 1598.37095068,\n        1640.5       , 1699.74483178, 1754.5       , 1678.66216081,\n        1608.59568089, 1597.3529073 , 1586.10394803, 1583.22844647,\n        1546.69107892, 1546.69107892, 1551.75321396, 1566.00424791,\n        1644.39610389, 1756.9285776 , 1730.56442135, 1709.77179391,\n        1709.77179391, 1614.1955481 , 1587.52017875],\n       [1455.21375514, 1575.27604551, 1575.27604551, 1599.53256267,\n        1640.62081942, 1698.20174727, 1751.41910472, 1677.26849104,\n        1640.85684295, 1641.        , 1579.08246903, 1567.93583724,\n        1481.        , 1481.        , 1573.65515874, 1834.5       ,\n        1866.73141181, 1913.        , 1829.69789637, 1764.        ,\n        1764.        , 1649.46638019, 1617.5       ],\n       [1475.41938245, 1647.        , 1647.        , 1646.44318767,\n        1645.5       , 1635.88570712, 1627.        , 1620.98642685,\n        1635.07557808, 1641.        , 1579.08246903, 1567.93583724,\n        1481.        , 1481.        , 1573.65515874, 1834.5       ,\n        1866.73141181, 1913.        , 1829.69789637, 1764.        ,\n        1764.        , 1649.46638019, 1617.5       ]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.read(12)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:04.812883Z",
     "start_time": "2023-11-26T15:30:04.740743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.indexes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:05.159395Z",
     "start_time": "2023-11-26T15:30:05.125096Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(len(trains)):\n",
    "    img = tifffile.imread(trains[i]).astype(float)\n",
    "    msk = tifffile.imread(masks[i]).astype(float)\n",
    "\n",
    "    reshaped_img = img.reshape(-1, 12) # (23, 23, 12) -> (23*23, 12)\n",
    "    flatten_mask = msk.reshape(-1,1)   # (23, 23) -> (23*23, 1)\n",
    "\n",
    "    is_train     = 1\n",
    "    if i % 5 == 0:  is_train     = 0\n",
    "    is_train_image = np.full((reshaped_img.shape[0], 1), is_train)\n",
    "\n",
    "    combine_data = np.hstack((reshaped_img, flatten_mask))\n",
    "    combine_data = np.hstack((combine_data, is_train_image))\n",
    "\n",
    "    data_list.append(combine_data)\n",
    "\n",
    "data_list = np.vstack(data_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:11.100180Z",
     "start_time": "2023-11-26T15:30:05.704809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                  B1           B2           B3           B4           B5  \\\n0         265.800583   279.066472   488.098252   331.572300   817.002914   \n1         265.800583   271.067287   472.501621   322.083156   817.002914   \n2         265.800583   240.942322   424.775329   279.931718   784.908433   \n3         265.800583   222.820502   411.740847   245.060190   737.346930   \n4         378.735906   229.331874   434.090056   254.714547   782.884060   \n...              ...          ...          ...          ...          ...   \n1133567  1166.000000  1049.347260  1190.897532  1254.509945  1365.000000   \n1133568  1166.000000  1090.473893  1243.402564  1362.630769  1365.000000   \n1133569  1089.344375  1097.961592  1251.727616  1360.179232  1399.362866   \n1133570   905.000000  1018.380712  1222.829764  1265.576354  1482.000000   \n1133571   905.000000   715.804236   983.997559   920.909463  1286.349768   \n\n                  B6           B7           B8           B9          B10  \\\n0        2400.280728  3084.914212  3236.613360  3332.798479  3251.691757   \n1        2400.280728  3084.914212  3188.111508  3332.798479  3251.691757   \n2        2322.216009  2998.675982  2906.248048  3251.978778  3251.691757   \n3        2206.530216  2870.877686  2640.218320  3132.210318  3251.691757   \n4        2226.587592  2840.185718  2651.574272  3084.218565  3341.757199   \n...              ...          ...          ...          ...          ...   \n1133567  1379.000000  1468.000000  1463.018392  1461.000000  1490.000000   \n1133568  1379.000000  1468.000000  1572.583217  1461.000000  1490.000000   \n1133569  1489.137392  1583.130287  1621.304781  1591.402672  1639.199454   \n1133570  1754.000000  1860.000000  1952.131222  1905.000000  1998.000000   \n1133571  1811.263483  1965.460247  2150.193046  2071.541295  1998.000000   \n\n                 B11          B12  Mask  is_train  \n0        1598.907575   818.413402   0.0       0.0  \n1        1598.907575   818.413402   0.0       0.0  \n2        1549.296809   798.364396   0.0       0.0  \n3        1475.777544   768.653340   0.0       0.0  \n4        1562.631939   809.818550   0.0       0.0  \n...              ...          ...   ...       ...  \n1133567  2027.000000  1859.000000   0.0       0.0  \n1133568  2027.000000  1859.000000   0.0       0.0  \n1133569  2009.671717  1797.029361   0.0       0.0  \n1133570  1968.000000  1648.000000   0.0       0.0  \n1133571  1812.434205  1389.359937   0.0       0.0  \n\n[1133572 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>B1</th>\n      <th>B2</th>\n      <th>B3</th>\n      <th>B4</th>\n      <th>B5</th>\n      <th>B6</th>\n      <th>B7</th>\n      <th>B8</th>\n      <th>B9</th>\n      <th>B10</th>\n      <th>B11</th>\n      <th>B12</th>\n      <th>Mask</th>\n      <th>is_train</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265.800583</td>\n      <td>279.066472</td>\n      <td>488.098252</td>\n      <td>331.572300</td>\n      <td>817.002914</td>\n      <td>2400.280728</td>\n      <td>3084.914212</td>\n      <td>3236.613360</td>\n      <td>3332.798479</td>\n      <td>3251.691757</td>\n      <td>1598.907575</td>\n      <td>818.413402</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>265.800583</td>\n      <td>271.067287</td>\n      <td>472.501621</td>\n      <td>322.083156</td>\n      <td>817.002914</td>\n      <td>2400.280728</td>\n      <td>3084.914212</td>\n      <td>3188.111508</td>\n      <td>3332.798479</td>\n      <td>3251.691757</td>\n      <td>1598.907575</td>\n      <td>818.413402</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>265.800583</td>\n      <td>240.942322</td>\n      <td>424.775329</td>\n      <td>279.931718</td>\n      <td>784.908433</td>\n      <td>2322.216009</td>\n      <td>2998.675982</td>\n      <td>2906.248048</td>\n      <td>3251.978778</td>\n      <td>3251.691757</td>\n      <td>1549.296809</td>\n      <td>798.364396</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>265.800583</td>\n      <td>222.820502</td>\n      <td>411.740847</td>\n      <td>245.060190</td>\n      <td>737.346930</td>\n      <td>2206.530216</td>\n      <td>2870.877686</td>\n      <td>2640.218320</td>\n      <td>3132.210318</td>\n      <td>3251.691757</td>\n      <td>1475.777544</td>\n      <td>768.653340</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>378.735906</td>\n      <td>229.331874</td>\n      <td>434.090056</td>\n      <td>254.714547</td>\n      <td>782.884060</td>\n      <td>2226.587592</td>\n      <td>2840.185718</td>\n      <td>2651.574272</td>\n      <td>3084.218565</td>\n      <td>3341.757199</td>\n      <td>1562.631939</td>\n      <td>809.818550</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1133567</th>\n      <td>1166.000000</td>\n      <td>1049.347260</td>\n      <td>1190.897532</td>\n      <td>1254.509945</td>\n      <td>1365.000000</td>\n      <td>1379.000000</td>\n      <td>1468.000000</td>\n      <td>1463.018392</td>\n      <td>1461.000000</td>\n      <td>1490.000000</td>\n      <td>2027.000000</td>\n      <td>1859.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1133568</th>\n      <td>1166.000000</td>\n      <td>1090.473893</td>\n      <td>1243.402564</td>\n      <td>1362.630769</td>\n      <td>1365.000000</td>\n      <td>1379.000000</td>\n      <td>1468.000000</td>\n      <td>1572.583217</td>\n      <td>1461.000000</td>\n      <td>1490.000000</td>\n      <td>2027.000000</td>\n      <td>1859.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1133569</th>\n      <td>1089.344375</td>\n      <td>1097.961592</td>\n      <td>1251.727616</td>\n      <td>1360.179232</td>\n      <td>1399.362866</td>\n      <td>1489.137392</td>\n      <td>1583.130287</td>\n      <td>1621.304781</td>\n      <td>1591.402672</td>\n      <td>1639.199454</td>\n      <td>2009.671717</td>\n      <td>1797.029361</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1133570</th>\n      <td>905.000000</td>\n      <td>1018.380712</td>\n      <td>1222.829764</td>\n      <td>1265.576354</td>\n      <td>1482.000000</td>\n      <td>1754.000000</td>\n      <td>1860.000000</td>\n      <td>1952.131222</td>\n      <td>1905.000000</td>\n      <td>1998.000000</td>\n      <td>1968.000000</td>\n      <td>1648.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1133571</th>\n      <td>905.000000</td>\n      <td>715.804236</td>\n      <td>983.997559</td>\n      <td>920.909463</td>\n      <td>1286.349768</td>\n      <td>1811.263483</td>\n      <td>1965.460247</td>\n      <td>2150.193046</td>\n      <td>2071.541295</td>\n      <td>1998.000000</td>\n      <td>1812.434205</td>\n      <td>1389.359937</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1133572 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data_list, columns= ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11','B12', 'Mask', 'is_train'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:11.105402Z",
     "start_time": "2023-11-26T15:30:11.100347Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0    905863\n0.0    227709\nName: is_train, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_train'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:11.140540Z",
     "start_time": "2023-11-26T15:30:11.110734Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "931"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.iloc[:,0:12]  #independent columns\n",
    "test= df.iloc[:,-2]    #target column i.e price range\n",
    "test\n",
    "del df\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:11.227454Z",
     "start_time": "2023-11-26T15:30:11.199944Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(11).plot(kind='barh')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import seaborn as sns\n",
    "corrmat = df.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(10,10))\n",
    "#plot heat map\n",
    "g=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10, 'Score'))  #print 10 best features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Splitter :\n",
    "    def __init__(self, kfold = True, n_splits = 5, df = pd.DataFrame(), test_size = 0.3 ):\n",
    "        self.n_splits = n_splits\n",
    "        self.kfold = kfold\n",
    "        self.df = df\n",
    "        self.test_size = test_size\n",
    "\n",
    "    def split_data(self, X, y, random_state_list):\n",
    "        if self.kfold == 'skf' :\n",
    "            for random_state in random_state_list :\n",
    "                kf = StratifiedKFold(n_splits = self.n_splits, random_State = random_state, shuffle = True)\n",
    "                for train_index, val_index in tqdm(kf.split(X, self.df)):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val, val_index\n",
    "\n",
    "        elif self.kfold:\n",
    "            for random_state in random_state_list :\n",
    "                kf = StratifiedKFold(n_splits = self.n_splits, random_State = random_state, shuffle = True)\n",
    "                for train_index, val_index in tqdm(kf.split(X, y)):\n",
    "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "                    yield X_train, X_val, y_train, y_val,val_index\n",
    "\n",
    "        else :\n",
    "            for random_state in random_state_list :\n",
    "                X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=self.test_size, random_state=random_state)\n",
    "                yield X_train, X_val, y_train, y_val\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:12.941175Z",
     "start_time": "2023-11-26T15:30:12.897607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "n_estimators = 50\n",
    "scoring = 'roc_auc'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:14.003004Z",
     "start_time": "2023-11-26T15:30:13.996004Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "splitter = Splitter(kfold=False, test_size=0.7)\n",
    "X_train, X_val, y_train, y_val = next(iter(splitter.split_data(train, test, random_state_list=[42])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:15.219354Z",
     "start_time": "2023-11-26T15:30:14.867931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, target_col, n_estimators=100, device=\"cpu\", random_state=0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.device = device\n",
    "        self.random_state = random_state\n",
    "        self.models = self._define_model(target_col)\n",
    "        self.models_name = list(self._define_model(target_col).keys())\n",
    "        self.len_models = len(self.models)\n",
    "\n",
    "    def _define_model(self, target_col):\n",
    "\n",
    "        if target_col == 'Mask':\n",
    "            xgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.00764887595631511,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 0.0379675844718388,\n",
    "                'alpha': 0.0127846822376181,\n",
    "                'subsample': 0.954836600053134,\n",
    "                'colsample_bytree': 0.303279712350842,\n",
    "                'max_depth': 9,\n",
    "                'min_child_weight': 8,\n",
    "                'eta': 7.78550117314348E-07,\n",
    "                'gamma': 2.16524511622072E-06,\n",
    "                'grow_policy': 'lossguide',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'logloss',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "            xgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.00610113589892878,\n",
    "                'booster': 'gbtree',\n",
    "                'lambda': 8.96906460616343E-08,\n",
    "                'alpha': 0.179699301214927,\n",
    "                'subsample': 0.693147524348261,\n",
    "                'colsample_bytree': 0.201796295242956,\n",
    "                'max_depth': 7,\n",
    "                'min_child_weight': 7,\n",
    "                'eta': 0.491486337405877,\n",
    "                'gamma': 0.0219744528297816,\n",
    "                'grow_policy': 'lossguide',\n",
    "                'n_jobs': -1,\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'logloss',\n",
    "                'verbosity': 0,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "        if self.device == 'gpu':\n",
    "            xgb1_params['tree_method'] = 'gpu_hist'\n",
    "            xgb1_params['predictor'] = 'gpu_predictor'\n",
    "            xgb2_params['tree_method'] = 'gpu_hist'\n",
    "            xgb2_params['predictor'] = 'gpu_predictor'\n",
    "\n",
    "        if target_col == 'Mask':\n",
    "            lgb1_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0228011747280739,\n",
    "                'reg_alpha': 5.65344226754958E-08,\n",
    "                'reg_lambda': 0.0000457329103646115,\n",
    "                'num_leaves': 41,\n",
    "                'colsample_bytree': 0.44294848432797,\n",
    "                'subsample': 0.751451025433351,\n",
    "                'subsample_freq': 4,\n",
    "                'min_child_samples': 47,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'logloss', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            lgb2_params = {\n",
    "                'n_estimators': self.n_estimators,\n",
    "                'learning_rate': 0.0150317388979459,\n",
    "                'reg_alpha': 0.000137387220669803,\n",
    "                'reg_lambda': 0.0465956586731766,\n",
    "                'num_leaves': 122,\n",
    "                'colsample_bytree': 0.577212162074888,\n",
    "                'subsample': 0.414797294649733,\n",
    "                'subsample_freq': 2,\n",
    "                'min_child_samples': 89,\n",
    "                'objective': 'binary',\n",
    "                'metric': 'logloss', # binary_error\n",
    "                'boosting_type': 'gbdt',\n",
    "                #'is_unbalance':True,\n",
    "                # 'n_jobs': -1,\n",
    "                #'force_row_wise': True,\n",
    "                'device': self.device,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "\n",
    "        if target_col == 'Mask':\n",
    "            cat1_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 3,\n",
    "                'learning_rate': 0.020258010893459,\n",
    "                'l2_leaf_reg': 0.583685138705941,\n",
    "                'random_strength': 0.177768021213223,\n",
    "                'od_type': \"Iter\",\n",
    "                'od_wait': 116,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Depthwise',\n",
    "                'bagging_temperature': 0.478048798393903,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            cat2_params = {\n",
    "                'iterations': self.n_estimators,\n",
    "                'depth': 4,\n",
    "                'learning_rate': 0.0533074594005429,\n",
    "                'l2_leaf_reg': 4.33121673696473,\n",
    "                'random_strength': 0.00420305570017096,\n",
    "                'od_type': \"IncToDec\",\n",
    "                'od_wait': 41,\n",
    "                'bootstrap_type': \"Bayesian\",\n",
    "                'grow_policy': 'Lossguide',\n",
    "                'bagging_temperature': 9.20357081888618,\n",
    "                'eval_metric': 'AUC', # AUC\n",
    "                'loss_function': 'Logloss',\n",
    "                #'auto_class_weights': 'Balanced',\n",
    "                'task_type': self.device.upper(),\n",
    "                'verbose': False,\n",
    "                'allow_writing_files': False,\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "\n",
    "        if target_col == 'Mask':\n",
    "            hist_params = {\n",
    "                'l2_regularization': 0.654926989031482,\n",
    "                'learning_rate': 0.0366207257406611,\n",
    "                'max_iter': self.n_estimators,\n",
    "                'max_depth': 30,\n",
    "                'max_bins': 255,\n",
    "                'min_samples_leaf': 52,\n",
    "                'max_leaf_nodes':12,\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 50,\n",
    "                #'class_weight':'balanced',\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            mlp_params = {\n",
    "                'max_iter': 800,\n",
    "                'early_stopping': True,\n",
    "                'n_iter_no_change': 50,\n",
    "                'random_state': self.random_state,\n",
    "            }\n",
    "\n",
    "        if target_col == 'Mask':\n",
    "            models = {\n",
    "                \"xgb\": xgb.XGBClassifier(**xgb1_params),\n",
    "                \"lgb\": lgb.LGBMClassifier(**lgb1_params),\n",
    "                \"cat\": CatBoostClassifier(**cat1_params),\n",
    "                #                 \"xgb2\": xgb.XGBClassifier(**xgb2_params),\n",
    "                \"lgb2\": lgb.LGBMClassifier(**lgb2_params),\n",
    "                #                 \"cat2\": CatBoostClassifier(**cat2_params),\n",
    "                'hgb': HistGradientBoostingClassifier(**hist_params),\n",
    "                'rfc': RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=self.random_state),\n",
    "                #'lrc': make_pipeline(StandardScaler(), LogisticRegression(max_iter=500, random_state=self.random_state)),\n",
    "                #                 'etc': ExtraTreesClassifier(n_estimators=800, n_jobs=-1, random_state=self.random_state),\n",
    "                #'mlp': make_pipeline(StandardScaler(), MLPClassifier(**mlp_params, hidden_layer_sizes=(100,))),\n",
    "                'ada': AdaBoostClassifier(n_estimators=100, random_state=self.random_state)\n",
    "                #'knn': KNeighborsClassifier(n_neighbors=15, n_jobs=-1),\n",
    "                #'gbc': GradientBoostingClassifier(n_estimators=500, random_state=self.random_state)\n",
    "                #'svc': SVC(max_iter=-1, kernel=\"rbf\", gamma=\"auto\", probability=True, random_state=self.random_state),\n",
    "            }\n",
    "\n",
    "\n",
    "        return models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:15.616644Z",
     "start_time": "2023-11-26T15:30:15.591382Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def plot_recursive_feature_elimination(elimination, scoring, min_features_to_select, name):\n",
    "    n_scores = len(elimination.cv_results_[\"mean_test_score\"])\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(f\"{scoring}\")\n",
    "\n",
    "    # Plot the mean test scores with error bars\n",
    "    plt.errorbar(\n",
    "        range(min_features_to_select, n_scores + min_features_to_select),\n",
    "        elimination.cv_results_[\"mean_test_score\"],\n",
    "        yerr=elimination.cv_results_[\"std_test_score\"],\n",
    "        fmt='o-',\n",
    "        capsize=3,\n",
    "        markersize=4,\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{name} Recursive Feature Elimination with correlated features\", fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:16.760093Z",
     "start_time": "2023-11-26T15:30:16.755748Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "0          0.0\n1          0.0\n2          0.0\n3          0.0\n4          0.0\n          ... \n1133567    0.0\n1133568    0.0\n1133569    0.0\n1133570    0.0\n1133571    0.0\nName: Mask, Length: 1133572, dtype: float64"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T15:30:17.545707Z",
     "start_time": "2023-11-26T15:30:17.540628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Mask--\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 50\n",
    "scoring = 'roc_auc'\n",
    "target_cols = ['Mask']\n",
    "target_col = 'Mask'\n",
    "min_features_to_select = len(X_train.columns) - 3\n",
    "\n",
    "print(f'--{target_col}--')\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "\n",
    "splitter = Splitter(kfold=False, test_size=0.5)\n",
    "X_train_, X_val, y_train_, y_val = next(iter(splitter.split_data(X_train, y_train, random_state_list=[42])))\n",
    "\n",
    "classifier = Classifier(target_col, n_estimators, device='cpu', random_state=0)\n",
    "models = classifier.models\n",
    "\n",
    "models_name = [_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)]\n",
    "trained_models = dict(zip(models_name, ['' for _ in range(classifier.len_models)]))\n",
    "unnecessary_features = dict(zip(models_name, [[] for _ in range(classifier.len_models)]))\n",
    "for name, model in models.items():\n",
    "    if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "            elimination = RFECV(\n",
    "                model,\n",
    "                step=1,\n",
    "                min_features_to_select=min_features_to_select,\n",
    "                cv=3,\n",
    "                scoring=scoring,\n",
    "                n_jobs= None)\n",
    "    elimination.fit(X_train_, y_train_)\n",
    "    unnecessary_feature = list(X_train.columns[~elimination.get_support()])\n",
    "    idx = np.argmax(elimination.cv_results_['mean_test_score'])\n",
    "    mean_score = elimination.cv_results_['mean_test_score'][idx]\n",
    "    std_score = elimination.cv_results_['std_test_score'][idx]\n",
    "    print(f'{blu}{name}{res} {red} Best Mean{res} {scoring} {red}{mean_score:.5f} ± {std_score:.5f}{res} | N_STEP {idx}')\n",
    "    print(f\"Best unnecessary_feature: {unnecessary_feature}\")\n",
    "    removed_features = [f for i, f in enumerate(X_train.columns) if elimination.support_[i] == False]\n",
    "    ranked_features = sorted(zip(X_train.columns, elimination.ranking_), key=lambda x: x[1])\n",
    "    removed_features_by_ranking = [f[0] for f in ranked_features if f[0] in removed_features][::-1]\n",
    "    print(\"Removed features:\", removed_features_by_ranking)\n",
    "    print(f'{\"-\" * 60}')\n",
    "    trained_models[f'{name}'] = (elimination)\n",
    "    unnecessary_features[f'{name}'].extend(unnecessary_feature)\n",
    "\n",
    "unnecessary_features = np.concatenate([_ for _ in unnecessary_features.values()])\n",
    "features = np.unique(unnecessary_features, return_counts=True)[0]\n",
    "counts = np.unique(unnecessary_features, return_counts=True)[1]\n",
    "drop_features = list(features[counts >= 2])\n",
    "print(\"Features recommended to be removed:\", drop_features)\n",
    "\n",
    "for name, elimination in trained_models.items():\n",
    "    plot_recursive_feature_elimination(elimination, scoring, min_features_to_select, name)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-26T15:30:18.288713Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Settings\n",
    "kfold = 'skf'\n",
    "n_splits = 5 # 10\n",
    "n_reapts = 3 # 1\n",
    "random_state = 42\n",
    "n_estimators = 9999 # 9999\n",
    "early_stopping_rounds = 200\n",
    "n_trials = 1000 # 3000\n",
    "verbose = False\n",
    "device = 'cpu'\n",
    "\n",
    "# Fix seed\n",
    "random.seed(random_state)\n",
    "random_state_list = random.sample(range(9999), n_reapts)\n",
    "# random_state_list = [42]\n",
    "# n_reapts = len(random_state_list)\n",
    "\n",
    "# metrics\n",
    "def auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred)\n",
    "metric = auc\n",
    "metric_name = metric.__name__.upper()\n",
    "\n",
    "# To calculate runtime\n",
    "def sec_to_minsec(t):\n",
    "    min_ = int(t / 60)\n",
    "    sec = int(t - min_*60)\n",
    "    return min_, sec\n",
    "\n",
    "# Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_training_process(lossfunc_key, eval_results_, best_iters_, early_stopping_rounds):\n",
    "\n",
    "    metric_score_folds = pd.DataFrame.from_dict(eval_results_).T\n",
    "    fit_rmsle = metric_score_folds.fit.apply(lambda x: x[lossfunc_key])\n",
    "    val_rmsle = metric_score_folds.val.apply(lambda x: x[lossfunc_key])\n",
    "\n",
    "    n_splits = len(metric_score_folds)\n",
    "    n_rows = math.ceil(n_splits / 3)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, 3, figsize=(20, n_rows * 4), dpi=150)\n",
    "    ax = axes.flatten()\n",
    "\n",
    "    for i, (f, v, best_iter) in enumerate(zip(fit_rmsle, val_rmsle, best_iters_)):\n",
    "        sns.lineplot(f, color='#B90000', ax=ax[i], label='fit')\n",
    "        sns.lineplot(v, color='#048BA8', ax=ax[i], label='val')\n",
    "        ax[i].legend()\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "        ax[i].set_title(f'Fold {i}', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "        color = ['#048BA8', '#90A6B1']\n",
    "        span_range = [[0, best_iter], [best_iter + 10, best_iter + early_stopping_rounds]]\n",
    "\n",
    "        for idx, sub_title in enumerate([f'Best\\nIteration: {best_iter}', f'Early\\n Stopping: {early_stopping_rounds}']):\n",
    "            ax[i].annotate(sub_title,\n",
    "                           xy=(sum(span_range[idx]) / 2, 4000),\n",
    "                           xytext=(0, 0),\n",
    "                           textcoords='offset points',\n",
    "                           va=\"center\",\n",
    "                           ha=\"center\",\n",
    "                           color=\"w\",\n",
    "                           fontsize=12,\n",
    "                           fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))\n",
    "            ax[i].axvspan(span_range[idx][0] - 0.4, span_range[idx][1] + 0.4, color=color[idx], alpha=0.07)\n",
    "\n",
    "        ax[i].set_xlim(0, best_iter + 20 + early_stopping_rounds)\n",
    "        ax[i].set_xlabel('Boosting Round', fontsize=12)\n",
    "        ax[i].set_ylabel(f'{lossfunc_key}', fontsize=12)\n",
    "        ax[i].legend(loc='upper right', title=lossfunc_key)\n",
    "\n",
    "    for j in range(i+1, n_rows * 3):\n",
    "        ax[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(fi):\n",
    "    fi_gain = fi[[col for col in fi.columns if col.startswith('gain')]].mean(axis=1)\n",
    "    fi_splt = fi[[col for col in fi.columns if col.startswith('split')]].mean(axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(18, 6), dpi=150)\n",
    "\n",
    "    # Split fi.\n",
    "    data_splt = fi_splt.sort_values(ascending=False)\n",
    "    data_splt = data_splt.head(20)\n",
    "    sns.barplot(x=data_splt.values, y=data_splt.index,\n",
    "                color='#1E90FF', linewidth=0.5, edgecolor=\"black\", ax=ax[0])\n",
    "    ax[0].set_title(f'Feature Importance \"Split\"', fontdict={'fontweight': 'bold'})\n",
    "    ax[0].set_xlabel(\"Importance\", fontsize=12)\n",
    "    ax[0].spines['right'].set_visible(False)\n",
    "    ax[0].spines['top'].set_visible(False)\n",
    "\n",
    "    # Gain fi.\n",
    "    data_gain = fi_gain.sort_values(ascending=False)\n",
    "    data_gain = data_gain.head(20)\n",
    "    sns.barplot(x=data_gain.values, y=data_gain.index,\n",
    "                color='#4169E1', linewidth=0.5, edgecolor=\"black\", ax=ax[1])\n",
    "    ax[1].set_title(f'Feature Importance \"Gain\"', fontdict={'fontweight': 'bold'})\n",
    "    ax[1].set_xlabel(\"Importance\", fontsize=12)\n",
    "    ax[1].spines['right'].set_visible(False)\n",
    "    ax[1].spines['top'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_estimators_ = 600\n",
    "\n",
    "test_preds_list = []\n",
    "val_scores = []\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "\n",
    "feature_importances_ = pd.DataFrame(index=X_train.columns)\n",
    "eval_results_ = {}\n",
    "best_iters_ = []\n",
    "oof = np.zeros((X_train.shape[0]))\n",
    "test_preds = np.zeros((X_val.shape[0]))\n",
    "\n",
    "splitter = Splitter(kfold=kfold, n_splits=n_splits, df=y_train)\n",
    "for i, (X_train_, X_val, y_train_, y_val, val_index) in enumerate(splitter.split_data(X_train, y_train, random_state_list=[0])):\n",
    "    fold = i % n_splits\n",
    "    m = i // n_splits\n",
    "\n",
    "        # XGB .train() requires xgboost.DMatrix.\n",
    "        # https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix\n",
    "    fit_set = xgb.DMatrix(X_train_, y_train_)\n",
    "    val_set = xgb.DMatrix(X_val, y_val)\n",
    "    watchlist = [(fit_set, 'fit'), (val_set, 'val')]\n",
    "\n",
    "        # Training.\n",
    "        # https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training\n",
    "    classifier = Classifier(target_col, n_estimators_, device)\n",
    "    xgb_params = classifier.models['xgb'].get_params()\n",
    "        # xgb_params = xgb.XGBClassifier(n_estimators=3000, learning_rate=0.01).get_params()\n",
    "\n",
    "    eval_results_[fold] = {}\n",
    "    model = xgb.train(\n",
    "            num_boost_round=xgb_params['n_estimators'],\n",
    "            params=xgb_params,\n",
    "            dtrain=fit_set,\n",
    "            evals=watchlist,\n",
    "            evals_result=eval_results_[fold],\n",
    "            verbose_eval=False,\n",
    "            callbacks=[xgb.callback.EarlyStopping(early_stopping_rounds, data_name='val', save_best=True)])\n",
    "\n",
    "    val_preds = model.predict(val_set)\n",
    "    test_preds += model.predict(xgb.DMatrix(X_val)) / n_splits\n",
    "\n",
    "    oof[val_index] = val_preds\n",
    "\n",
    "    val_score = metric(y_val, val_preds)\n",
    "    best_iter = model.best_iteration\n",
    "    best_iters_.append(best_iter)\n",
    "    val_scores.append(val_score)\n",
    "    print(f'Fold: {blu}{fold:>3}{res}| {metric_name}: {blu}{val_score:.5f}{res}' f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "        # Stores the feature importances\n",
    "    feature_importances_[f'gain_{fold}'] = feature_importances_.index.map(model.get_score(importance_type='gain'))\n",
    "    feature_importances_[f'split_{fold}'] = feature_importances_.index.map(model.get_score(importance_type='weight'))\n",
    "\n",
    "\n",
    "test_preds_list.append(test_preds) # np.round(test_preds)\n",
    "    # xgb_test_preds = test_preds.copy()\n",
    "\n",
    "mean_cv_score_full = metric(y_train, oof)\n",
    "print(f'{\"*\" * 50}\\n{red}Mean full{res} {metric_name} : {red}{mean_cv_score_full:.5f}{res}')\n",
    "print(f'{red}Mean val{res} {metric_name}  : {red}{np.mean(val_scores):.5f}{res}')\n",
    "\n",
    "plot_training_process('auc', eval_results_, best_iters_, early_stopping_rounds)\n",
    "plot_feature_importance(feature_importances_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class OptunaWeights:\n",
    "    def __init__(self, random_state, n_trials=100):\n",
    "        self.study = None\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.n_trials = n_trials\n",
    "\n",
    "    def _objective(self, trial, y_true, y_preds):\n",
    "        # Define the weights for the predictions from each model\n",
    "        weights = [trial.suggest_float(f\"weight{n}\", 1e-15, 1) for n in range(len(y_preds))]\n",
    "\n",
    "        # Calculate the weighted prediction\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
    "\n",
    "        # Calculate the score for the weighted prediction\n",
    "        score = metric(y_true, weighted_pred)\n",
    "        return score\n",
    "\n",
    "    def fit(self, y_true, y_preds):\n",
    "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
    "        pruner = optuna.pruners.HyperbandPruner()\n",
    "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
    "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
    "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
    "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
    "\n",
    "    def predict(self, y_preds):\n",
    "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
    "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
    "        return weighted_pred\n",
    "\n",
    "    def fit_predict(self, y_true, y_preds):\n",
    "        self.fit(y_true, y_preds)\n",
    "        return self.predict(y_preds)\n",
    "\n",
    "    def weights(self):\n",
    "        return self.weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predss_list = []\n",
    "oof_predss_list = []\n",
    "ensemble_score_list = []\n",
    "weights_list = []\n",
    "trained_models_list = []\n",
    "score_dict_list = []\n",
    "X_val_list = []\n",
    "oof_preds_list = []\n",
    "\n",
    "for target_col, X_train, y_train, X_test in zip(target_cols, X_train, y_train, X_test):\n",
    "    print(f'=== {target_col} ===')\n",
    "    # drop_target_col = 'EC2' if target_col == 'EC1' else 'EC1'\n",
    "    # X_train = X_train.drop(drop_target_col, axis=1)\n",
    "\n",
    "    # Initialize an array for storing test predictions\n",
    "    classifier = Classifier(target_col, n_estimators, device, random_state)\n",
    "    test_predss = np.zeros((X_test.shape[0]))\n",
    "    oof_predss = np.zeros((X_train.shape[0], n_reapts))\n",
    "    ensemble_score, ensemble_score_ = [], []\n",
    "    weights = []\n",
    "    trained_models = dict(zip([_ for _ in classifier.models_name if ('xgb' in _) or ('lgb' in _) or ('cat' in _)], [[] for _ in range(classifier.len_models)]))\n",
    "    score_dict = dict(zip(classifier.models_name, [[] for _ in range(classifier.len_models)]))\n",
    "\n",
    "    splitter = Splitter(kfold=kfold, n_splits=n_splits, f=y_train)\n",
    "    for i, (X_train_, X_val, y_train_, y_val, val_index) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
    "        n = i % n_splits\n",
    "        m = i // n_splits\n",
    "\n",
    "        # Get a set of classifier models\n",
    "        classifier = Classifier(target_col, n_estimators, device, random_state_list[m])\n",
    "        models = classifier.models\n",
    "\n",
    "        # Initialize lists to store oof and test predictions for each base model\n",
    "        oof_preds = []\n",
    "        test_preds = []\n",
    "\n",
    "        # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
    "        for name, model in models.items():\n",
    "            best_iteration = None\n",
    "            start_time = time.time()\n",
    "\n",
    "            if ('xgb' in name) or ('lgb' in name) or ('cat' in name):\n",
    "                early_stopping_rounds_ = int(early_stopping_rounds*2) if ('lgb' in name) else early_stopping_rounds\n",
    "\n",
    "                if 'lgb' in name:\n",
    "                    model.fit(\n",
    "                        X_train_, y_train_, eval_set=[(X_val, y_val)],\n",
    "                        early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "                elif 'cat' in name :\n",
    "                    model.fit(\n",
    "                        Pool(X_train_, y_train_), eval_set=Pool(X_val, y_val, ),\n",
    "                        early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "                else:\n",
    "                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds_, verbose=verbose)\n",
    "\n",
    "                best_iteration = model.best_iteration if ('xgb' in name) else model.best_iteration_\n",
    "            else:\n",
    "                model.fit(X_train_, y_train_)\n",
    "\n",
    "            end_time = time.time()\n",
    "            min_, sec = sec_to_minsec(end_time - start_time)\n",
    "\n",
    "            if name in trained_models.keys():\n",
    "                trained_models[f'{name}'].append(deepcopy(model))\n",
    "\n",
    "            y_val_pred = model.predict_proba(X_val)[:, 1].reshape(-1)\n",
    "            test_pred = model.predict_proba(X_test)[:, 1].reshape(-1)\n",
    "\n",
    "            score = metric(y_val, y_val_pred)\n",
    "            score_dict[name].append(score)\n",
    "            print(f'{blu}{name}{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {blu}{score:.5f}{res} | Best iteration {blu}{best_iteration}{res} | Runtime {min_}min {sec}s')\n",
    "\n",
    "            oof_preds.append(y_val_pred)\n",
    "            test_preds.append(test_pred)\n",
    "\n",
    "        # Use Optuna to find the best ensemble weights\n",
    "        optweights = OptunaWeights(random_state=random_state_list[m], n_trials=n_trials)\n",
    "        y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
    "\n",
    "        score = metric(y_val, y_val_pred)\n",
    "        print(f'{red}>>> Ensemble{res} [FOLD-{n} SEED-{random_state_list[m]}] {metric_name} {red}{score:.5f}{res}')\n",
    "        print(f'{\"-\" * 60}')\n",
    "        ensemble_score.append(score)\n",
    "        weights.append(optweights.weights)\n",
    "\n",
    "        # Predict to X_test by the best ensemble weights\n",
    "        test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
    "        oof_predss[X_val.index, m] += optweights.predict(oof_preds)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    test_predss_list.append(test_predss)\n",
    "    oof_predss_list.append(oof_predss)\n",
    "    ensemble_score_list.append(ensemble_score)\n",
    "    weights_list.append(weights)\n",
    "    trained_models_list.append(trained_models)\n",
    "    score_dict_list.append(score_dict)\n",
    "    X_val_list.append(X_val)\n",
    "    oof_preds_list.append(oof_preds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_score_from_dict(score_dict, title='', ascending=True):\n",
    "    score_df = pd.melt(pd.DataFrame(score_dict))\n",
    "    score_df = score_df.sort_values('value', ascending=ascending)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='value', y='variable', data=score_df, palette='Blues_r', errorbar='sd')\n",
    "    plt.xlabel(f'{title}', fontsize=14)\n",
    "    plt.ylabel('')\n",
    "    #plt.title(f'{title}', fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, axis='x')\n",
    "    plt.show()\n",
    "\n",
    "for target_col, score_dict in zip(target_cols, score_dict_list):\n",
    "    print(f'=== {target_col} ===')\n",
    "    print(f'--- Mean {metric_name} Scores---')\n",
    "    for name, score in score_dict.items():\n",
    "        mean_score = np.mean(score)\n",
    "        std_score = np.std(score)\n",
    "        print(f'{name}: {red}{mean_score:.5f} ± {std_score:.5f}{res}')\n",
    "    plot_score_from_dict(score_dict, title=f'{metric_name} (n_splits:{n_splits})')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_heatmap_with_dendrogram(df, title, figsize=(18, 10), fontsize=10):\n",
    "    mask = np.zeros_like(df.astype(float).corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    colormap = plt.cm.RdBu_r\n",
    "    fig, ax = plt.subplots(2, 1, figsize=figsize)\n",
    "\n",
    "    # Plot heatmap\n",
    "    ax[0].set_title(f'{title} Correlation of Features', fontweight='bold', y=1.02, size=15)\n",
    "    sns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1.0,\n",
    "                square=True, cmap=colormap, linecolor='white', annot=True,\n",
    "                annot_kws={\"size\": fontsize, \"weight\": \"bold\"}, mask=mask, ax=ax[0], cbar=False)\n",
    "\n",
    "    # Plot dendrogram\n",
    "    correlations = df.corr()\n",
    "    converted_corr = 1 - np.abs(correlations)\n",
    "    Z = linkage(squareform(converted_corr), 'complete')\n",
    "\n",
    "    dn = dendrogram(Z, labels=df.columns, above_threshold_color='#ff0000', ax=ax[1])\n",
    "    ax[1].set_title(f'{title} Hierarchical Clustering Dendrogram', fontsize=15, fontweight='bold')\n",
    "    ax[1].grid(axis='x')\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "    ax[1].tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for target_col, ensemble_score, weights, oof_preds, score_dict in zip(target_cols, ensemble_score_list, weights_list, oof_preds_list, score_dict_list):\n",
    "    print(f'=== {target_col} ===')\n",
    "\n",
    "    # Calculate the mean LogLoss score of the ensemble\n",
    "    mean_score = np.mean(ensemble_score)\n",
    "    std_score = np.std(ensemble_score)\n",
    "    print(f'{red}Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')\n",
    "\n",
    "    print('')\n",
    "    # Print the mean and standard deviation of the ensemble weights for each model\n",
    "    print('--- Optuna Weights---')\n",
    "    mean_weights = np.mean(weights, axis=0)\n",
    "    std_weights = np.std(weights, axis=0)\n",
    "    for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
    "        print(f'{name}: {blu}{mean_weight:.5f} ± {std_weight:.5f}{res}')\n",
    "\n",
    "    # Plot Optuna Weights\n",
    "    normalize = [((weight - np.min(weight)) / (np.max(weight) - np.min(weight))).tolist() for weight in weights]\n",
    "    weight_dict = dict(zip(list(score_dict.keys()), np.array(normalize).T.tolist()))\n",
    "    plot_score_from_dict(weight_dict, title='Optuna Weights (Normalize 0 to 1)', ascending=False)\n",
    "\n",
    "    # Plot oof_predict analyis for each model\n",
    "    plot_heatmap_with_dendrogram(pd.DataFrame(oof_preds, index=list(score_dict.keys())).T, title='OOF Predict', figsize=(10, 10), fontsize=8)\n",
    "\n",
    "mean_score = np.mean(ensemble_score_list)\n",
    "std_score = np.std(ensemble_score_list)\n",
    "print(f'{red} EC1 and EC2 Mean{res} Optuna Ensemble {metric_name} {red}{mean_score:.5f} ± {std_score:.5f}{res}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df = df[['B6' ,'B12', 'B9', 'B7', 'B2', 'B8', 'B11', 'Mask']]\n",
    "#df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#data = df[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11','B12']]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "new_df = df[['B1', 'B2' ,'B6', 'B8', 'B9', 'B10']]\n",
    "\n",
    "new_df['Mask'] = df['Mask']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "figure1 = plt.figure(figsize=(10,7), facecolor='w')\n",
    "plt.axis('off')\n",
    "x = y.value_counts()\n",
    "print(x)\n",
    "\n",
    "plt.pie(x, labels=[i for i in x.index], autopct=\"%.1f%%\", pctdistance=0.8,\n",
    "        textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "plt.legend(['Class 0', 'Class 1'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "new_df.loc[:,'Mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X = new_df.loc[:,'B1':'B10']\n",
    "y = new_df.loc[:,'Mask']\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from optuna.integration import CatBoostPruningCallback\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import optuna\n",
    "from optuna import create_study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#X_train.reset_index(drop = True, inplace = True)\n",
    "##y_train.reset_index(drop = True, inplace = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#y_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sm = SMOTE(random_state=1945)\n",
    "X,y = sm.fit_resample(X, y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "logloss = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma': 5\n",
    "}\n",
    "for train_index, test_index in tqdm(skf.split(X, y)):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    # Create the DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    model = xgb.train(params, dtrain,\n",
    "                      evals=[(dtest, 'Validation')],\n",
    "\n",
    "                      verbose_eval=False)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(dtest)\n",
    "    pred_labels = np.rint(y_pred)\n",
    "    print(y_test.shape)\n",
    "    print(y_pred.shape)\n",
    "    print(y_pred[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "N_TRIALS = 50\n",
    "RS = 0  # random state\n",
    "N_JOBS = -1  # number of parallel threads\n",
    "\n",
    "# repeated K-folds\n",
    "N_SPLITS = 2\n",
    "N_REPEATS = 5\n",
    "\n",
    "# Optuna\n",
    "MULTIVARIATE = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(\n",
    "        trial,\n",
    "        X,\n",
    "        y,\n",
    "        random_state=0,\n",
    "        n_splits=N_SPLITS ,\n",
    "        n_repeats=N_REPEATS,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=20,\n",
    "):\n",
    "    # Catboost parameters\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1),\n",
    "        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n",
    "        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.50, 1)\n",
    "        param['min_child_weight'] = trial.suggest_int('min_child_weight', 5, 10)\n",
    "        param['colsample_bytree']= trial.suggest_float('colsample_bytree', 0.50, 1)\n",
    "        param['max_depth']= trial.suggest_int('max_depth', 1, 20)\n",
    "        param[\"eta\"] = trial.suggest_loguniform(\"eta\", 1e-8, 1.0)\n",
    "        param[\"gamma\"] = trial.suggest_loguniform(\"gamma\", 1e-8, 1.0)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_loguniform(\"rate_drop\", 1e-8, 1.0)\n",
    "        param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n",
    "\n",
    "    oversample = RandomUnderSampler(sampling_strategy=0.5)\n",
    "    X,y = oversample.fit_resample(X, y)\n",
    "\n",
    "\n",
    "    logLoss = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in tqdm(skf.split(X, y)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Create the DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "        early_stop = xgb.callback.EarlyStopping(rounds=20,min_delta=1e-3,\n",
    "                                                metric_name='logloss',\n",
    "                                                data_name='Train')\n",
    "\n",
    "        # Train the model with the current hyperparameters\n",
    "        model = xgb.train(param, dtrain,\n",
    "                          evals=[(dtrain, 'Train'), (dtest, 'Valid')],\n",
    "                          callbacks=[early_stop],\n",
    "                          verbose_eval=False)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(dtest)\n",
    "\n",
    "        # Convert probabilities to binary predictions\n",
    "        pred_labels = np.rint(y_pred)\n",
    "\n",
    "        ll= log_loss(y_test, pred_labels)\n",
    "        logLoss.append(ll)\n",
    "\n",
    "    mean_of_logloss = sum(logLoss) / len(logLoss)\n",
    "    print(\"mean_of_logloss :\", mean_of_logloss)\n",
    "    return mean_of_logloss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prep_data(X,y):\n",
    "    study = optuna.create_study(direction=\"minimize\",pruner=optuna.pruners.HyperbandPruner(),sampler=optuna.samplers.TPESampler(seed=10))\n",
    "\n",
    "    study.optimize(lambda trial :objective(trial,X, y,\n",
    "                                           random_state=RS,\n",
    "                                           n_splits=N_SPLITS,\n",
    "                                           n_repeats=N_REPEATS,\n",
    "                                           n_jobs=-1,\n",
    "                                           ),\n",
    "                   n_trials=N_TRIALS,\n",
    "                   n_jobs=-1,\n",
    "                   )\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    best_par = study.best_params\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    best_params = trial.params\n",
    "    best_params['objective'] = 'binary:logistic'\n",
    "\n",
    "\n",
    "    return best_par"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Apply StandardScaler to the input features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Convert the scaled data back to pandas DataFrame\n",
    "X = pd.DataFrame(X_scaled, columns=X.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def final_class(X,y ):\n",
    "\n",
    "    best_param=prep_data(X,y)\n",
    "    return best_param"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_params = final_class(X, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "optimal_mask_model = xgb.XGBClassifier(**best_params)\n",
    "optimal_mask_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "\n",
    "y_pred_prob_list = list()\n",
    "result3= []\n",
    "def modelPipelineGrid(X_train, X_test, y_train, y_test, step1):\n",
    "\n",
    "    xgb_params = {\n",
    "        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        'learning_rate': 0.00764887595631511,\n",
    "        'lambda': 0.0379675844718388,\n",
    "        'alpha': 0.0127846822376181,\n",
    "        'subsample': 0.954836600053134,\n",
    "        'colsample_bytree': 0.303279712350842,\n",
    "        'max_depth': 9,\n",
    "        'min_child_weight': 8,\n",
    "        'eta': 7.78550117314348E-07,\n",
    "        'gamma': 2.16524511622072E-06,\n",
    "        'grow_policy': 'lossguide',\n",
    "        \"seed\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\":\"logloss\",\n",
    "\n",
    "    }\n",
    "\n",
    "    if step1 == 'over' :\n",
    "        Imb_Method = SMOTE(sampling_strategy = 0.5, random_state=42)\n",
    "    if step1 == 'under' :\n",
    "        Imb_Method = RandomUnderSampler(random_state=42, sampling_strategy = 'auto',replacement=True)\n",
    "\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "            ('step1', Imb_Method),\n",
    "            ('classifier', XGBClassifier(**xgb_params))\n",
    "        ]\n",
    "        )\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    time_1 = time.time()\n",
    "    y_model_pred = model.predict(X_test)\n",
    "    time_2 = time.time()\n",
    "    y_model_pred_prob = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "    result3.append({'model': model, 'F1_score': f1_score(y_test, y_model_pred ),\n",
    "                        'Precision': precision_score(y_test,y_model_pred ), 'Recall': recall_score(y_test, y_model_pred ),\n",
    "                        'Accuracy': accuracy_score(y_test, y_model_pred), 'ROC_AUC':roc_auc_score(y_test, y_model_pred_prob),\n",
    "                        'fbeta_Score' : fbeta_score(y_test, y_model_pred, average = 'binary', beta=2),\n",
    "                         'Predict Time(s)': time_2 - time_1})\n",
    "    y_pred_prob_list.append(y_model_pred_prob)\n",
    "\n",
    "    return y_pred_prob_list, model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " prob_list,model = modelPipelineGrid(X_train, X_test, y_train, y_test, 'over')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "y_pred = optimal_mask_model.predict(X_test)\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cf_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cf_matrix):\n",
    "    plt.figure(figsize=(9, 7))\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='nipy_spectral')\n",
    "\n",
    "    plt.title(\"Prédiction avec les données normalisées\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cf_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_path =  'evaluation/'\n",
    "test_mask_path = 'sample/'\n",
    "#test_path =  '/tmp/kaggledata/solafue_solars/train/s2_image/'\n",
    "#test_mask_path = '/tmp/kaggledata/solafue_solars/train/mask/'\n",
    "\n",
    "masks = glob.glob(f'{test_mask_path}/*')\n",
    "tests = glob.glob(f'{test_path}/*')\n",
    "masks.sort()\n",
    "tests.sort()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isdir('output'):\n",
    "    os.mkdir('output')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i, (m, t) in tqdm(enumerate(zip(masks, tests))):\n",
    "\n",
    "    basename = os.path.basename(m)\n",
    "    output_file = f'output/{basename}'\n",
    "\n",
    "    img = tifffile.imread(t).astype(np.float)\n",
    "    mask = tifffile.imread(m).astype(np.float)\n",
    "    img = img.reshape(-1, 12)\n",
    "\n",
    "    img = pd.DataFrame(data=img, columns= ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11','B12'])\n",
    "\n",
    "    img = img[['B1', 'B2' ,'B6', 'B8', 'B9', 'B10']]\n",
    "\n",
    "    img = scaler.transform(img)\n",
    "\n",
    "    shape_mask = mask.shape\n",
    "\n",
    "    pred = optimal_mask_model.predict_proba(img)\n",
    "    pred_mask = np.argmax(pred, axis=1).astype(np.uint8)\n",
    "    pred_mask = pred_mask.reshape(shape_mask[0], shape_mask[1])\n",
    "\n",
    "    tifffile.imwrite(output_file, pred_mask)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('output_xgb_optuna', 'zip', 'output')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
